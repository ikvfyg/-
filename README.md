# Python网络爬虫说明文档

## 第九组 爬虫B

### 设计文档

#### 设计任务

**具体描述：** 从[NeurIPS会议网站](https://proceedings.neurips.cc/)爬取所有年份（1987-2022）的论文的标题、作者和论文摘要，将这些信息写入Word文档；将爬取的信息制作一个新的网页（离线网页即可）；针对论文摘要中的关键词绘制词云；统计在此会议中发表论文数大于5篇的作者并对他们按照论文数排序。

#### 任务分析

需要分析网页的XML框架，再爬取相关论文的信息，同时需要创建word文档，将相关信息写入word文档，由于文档数过多，考虑使用多进程处理。然后通过年份筛选，将各个年份的论文分类处理如排序，挑选出发表数大于5篇的作者，根据关键词制作词云，在制作一个网页的基本框架，将图形可视化的词云填入，论文信息填入其中

#### 小组成员

- 组长：09022130蔡思宇
- 组员：
  - 09022101马顺道
  - 71122204钟淙宇
  - 58122128陈俊霖
  - 58122126徐演秋

#### 设计思路与流程

##### 一. 论文的爬取

1. **爬虫框架和库：** 使用requests库进行HTTP请求，从而获取网页内容。使用beautifulsoup4库解析HTML内容，提取所需的信息。
   
2. **Word文档处理：** 使用python-docx库创建和编辑Word文档，将论文信息添加到文档中。
   
3. **多进程处理：** 利用Python的multiprocessing库，使用多进程并发处理多个年份的数据，以提高效率。
   
4. **网络请求和错误处理：** 针对网络请求设置重试机制，以处理可能的连接问题。在发生请求异常时，通过requests.exceptions.RequestException进行适当的错误处理。

5. **文本清理和处理：** 使用正则表达式清理文本，以移除非XML兼容字符，确保提取的信息是干净的。

6. **配置和定制：** 通过设置years_to_scrape列表，用户可以自定义要爬取的年份。

7. **数据存储和处理：** 引入pandas库，将爬取的论文信息存储为Excel文件。

8. **Excel文件保存：** 将数据字典转换为pandas的DataFrame，使用to_excel方法将DataFrame保存为Excel文件。

##### 二. 对论文进行排序以及统计结果

1. **获取源文件：** 读取一个原始的CSV文件，该文件包含作者发表论文的信息。

2. **读取文件并保存：** 使用`pandas`库的`read_csv`函数读取CSV文件并将其存储在名为`df`的DataFrame对象中。

3. **统计数量：** 通过使用`value_counts`函数统计每位作者发表的论文数，并将结果存储在`authors_counts`对象中。

4. **筛选：** 使用条件筛选的方式，筛选出发表论文数大于4篇的作者，并将结果存储在`authors_over_4`对象中。

5. **排序：** 使用`sort_values`函数按照论文数进行降序排序，得到最终的`authors_sorted`对象。

6. **保存文件：** 通过调用`to_csv`函数，将排序结果保存为一个新的CSV文件`authors_sorted_more_than_4.csv`，并将索引列去掉（`index=False`）。

##### 三. 制作词云

1. **导入库：** 使用pandas进行数据处理，WordCloud生成词云，matplotlib.pyplot进行绘图，tqdm显示进度条，time模拟耗时操作。

2. **读取CSV文件：** 使用pandas读取名为'papers_202312192140.csv'的CSV文件，并将其存储在一个DataFrame（df）中。

3. **提取摘要：** 从DataFrame中提取'abstract'列，去除任何NaN值，并将其转换为摘要的列表（abstracts）。

4. **设置进度更新间隔：** 设置更新进度条的间隔（progress_interval），并计算总行数。

5. **生成词云：** 创建一个WordCloud对象，设置特定参数，然后从摘要中生成词云。

6. **显示进度：** 使用tqdm迭代行，在一定间隔内显示进度条，包括一个模拟数据处理的时间延迟。

7. **显示词云：** 使用Matplotlib显示生成的词云图。

8. **保存词云图像：** 将生成的词云图保存到名为'wordcloud_output.png'的本地文件中。

9. **打印成功消息：** 打印一条消息，指示词云图已成功保存。

##### 四. 制作网页

1. **路由和视图函数：** 使用 @app.route 装饰器定义了几个路由，每个路由对应一个视图函数，决定了不同URL请求的响应行为。

2. **数据库连接和查询：** 在 title() 视图函数中，使用 SQLite 数据库连接，执行 SQL 查询，获取数据，并将数据传递给相应的模板。

3. **模板渲染：** 使用 render_template 函数渲染 HTML 模板，并将查询结果或其他数据传递给模板中的变量。

4. **静态文件目录结构：** templates 中包含了 HTML 模板文件。

5. **Flask应用运行：** if __name
__ == '__main__': app.run() 语句确保在直接运行这个脚本时，才会启动 Flask 应用。

该代码通过 Flask 提供了简单的路由和视图函数机制，使得可以根据不同的URL请求返回不同的HTML页面。

涉及到数据库连接的部分使用了 SQLite。

可以通过 Flask 提供的 render_template 函数将数据传递给 HTML 模板，实现动态网页的构建。

### 功能介绍

具体使用说明请参考文件中的“使用说明.txt”。

#### 制作出来的网页如下：

1. **根据我们按照年份爬取到的论文：**

2. **对网页上汇总的论文也按照年份分类：**

3. **点击年份按钮，每年的论文信息如下：**

4. **按照年份绘制的词云：**

5. **点击开2001年按钮，词云结果如下：**

6. **统计出论文发表数量大于4篇的作者以及不少于3篇的作者：**

7. **统计出论文发表数量大于5篇的作者以及不少于3篇的作者：**

### 总结

这个项目是一个非常有挑战性的任务，涉及到了多个技术领域，包括网络爬虫、数据处理、网页制作、数据可视化和统计分析。在完成项目的过程中，我们组遇到了许多困难和挑战，但也获得了很多有意义的收获。

首先，爬取论文信息是一个重要的步骤。从网站上获取数据并解析需要一定的HTML和数据处理知识。我们组学习了如何使用Python的requests和BeautifulSoup库来发送请求、解析HTML页面，并从中提取所需的数据。这使我们对网络爬虫的原理和技术有了更深入的理解。

其次，将论文信息写入Word文档需要熟悉操作Word文档的库和技巧。我们组学习了如何使用Python的Python-docx库来创建和编辑Word文档，将爬取到的论文信息逐个添加到文档中。这让我们更加熟悉了文档处理的过程和技术。

制作一个新的网页并实现数据可视化是这个项目的亮点之一。使用Bootstrap和Flask来创建网页，使用JavaScript和D3.js来制作词云的过程让我们组学习了如何使用这些强大的工具和库来实现交互式和动态的数据可视化。我们组也了解到了数据可视化在信息传达和展示方面的重要性。

最后，使用Python的pandas库对论文作者进行统计和排序是一个很有挑战性的任务。我们组学习了如何使用pandas进行数据分组、计数和排序，并将统计结果可视化展示。这让我更加熟悉了数据分析和统计的方法，以及pandas库的强大功能。

通过完成这个项目，我们组不仅加深了对爬虫、网页制作、数据可视化和统计分析的理解，还提高了自己的问题解决能力和技术实践能力。认识到数据在现代社会中的重要性，以及数据处理和可视化技术在从大量数据中提取有用信息方面的价值。

总的来说，这个项目是非常有挑战性但充满成就感的。我们组通过它学到了许多知识和技能，并对自己在数据处理和可视化方面的能力有了更深入和全面的认识。这个项目也进一步激发了学生对数据科学和数据分析领域的兴趣，期待在未来能够继续深入学习和应用这些知识。

### 贡献比例

- 58122126徐演秋：21%
- 09022130蔡思宇：20%
- 09022101马顺道：20%
- 71122204钟淙宇：20%
- 58122128陈俊霖：19%

